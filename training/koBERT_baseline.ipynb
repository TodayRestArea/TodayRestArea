{"cells":[{"cell_type":"markdown","source":["# Kobert 사용 baseline"],"metadata":{"id":"Q1y7YRayN5MS"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22258,"status":"ok","timestamp":1642892380347,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"},"user_tz":-540},"id":"-0v18kTD0Wn-","outputId":"5ae5c80d-b4fa-4d49-ae91-03b3f196323e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n","Collecting transformers==3\n","  Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.62.3)\n","Collecting tokenizers==0.8.0-rc4\n","  Using cached tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (21.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.4.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.96)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.47)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (3.0.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.10.3\n","    Uninstalling tokenizers-0.10.3:\n","      Successfully uninstalled tokenizers-0.10.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.15.0\n","    Uninstalling transformers-4.15.0:\n","      Successfully uninstalled transformers-4.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kobert 0.2.3 requires transformers>=4.8.1, but you have transformers 3.0.0 which is incompatible.\u001b[0m\n","Successfully installed tokenizers-0.8.0rc4 transformers-3.0.0\n","Requirement already satisfied: gluonnlp in /usr/local/lib/python3.7/dist-packages (0.10.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.26)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.6)\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-t280fbke\n","  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-t280fbke\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.20.41)\n","Requirement already satisfied: gluonnlp>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.10.0)\n","Requirement already satisfied: mxnet>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.9.0)\n","Requirement already satisfied: onnxruntime==1.8.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.8.0)\n","Requirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.1.96)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.10.0+cu111)\n","Collecting transformers>=4.8.1\n","  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.17.3)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (2.0)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (21.3)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.26)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (2.23.0)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (0.8.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2021.10.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.2.3) (3.10.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.62.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Using cached tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.10.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (0.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (3.4.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (0.0.47)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.2.3) (3.0.6)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->kobert==0.2.3) (0.10.0)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->kobert==0.2.3) (0.5.0)\n","Requirement already satisfied: botocore<1.24.0,>=1.23.41 in /usr/local/lib/python3.7/dist-packages (from boto3->kobert==0.2.3) (1.23.41)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.41->boto3->kobert==0.2.3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.41->boto3->kobert==0.2.3) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.2.3) (3.7.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.1.0)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.8.0rc4\n","    Uninstalling tokenizers-0.8.0rc4:\n","      Successfully uninstalled tokenizers-0.8.0rc4\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 3.0.0\n","    Uninstalling transformers-3.0.0:\n","      Successfully uninstalled transformers-3.0.0\n","Successfully installed tokenizers-0.10.3 transformers-4.15.0\n","Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.9)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.3)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n","Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.2.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.26)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n","Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n","Requirement already satisfied: pytorchtools in /usr/local/lib/python3.7/dist-packages (0.0.2)\n"]}],"source":["!pip install konlpy\n","!pip install transformers==3\n","!pip install gluonnlp pandas tqdm\n","!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n","!pip install wandb\n","!pip install pytorchtools"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5246,"status":"ok","timestamp":1642892385573,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"},"user_tz":-540},"id":"9XMZQmq_HhF2"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import ast\n","from konlpy.tag import Okt, Hannanum, Komoran, Kkma\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import gluonnlp as nlp\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","import wandb\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","import random\n","import copy\n","import time\n","import gc\n","import re\n","import torch\n","\n","import spacy\n","from tqdm import tqdm_notebook, tnrange\n","from tqdm.auto import tqdm\n","\n","tqdm.pandas(desc='Progress')\n","from collections import Counter\n","\n","from nltk import word_tokenize\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.autograd import Variable\n","from sklearn.metrics import f1_score\n","import os \n","from tensorflow import keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","from keras.layers import Dropout\n","from torch.optim import Adam\n","\n","# cross validation and metrics\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import f1_score\n","from torch.optim.optimizer import Optimizer\n","\n","from sklearn.preprocessing import StandardScaler\n","from multiprocessing import  Pool\n","from functools import partial\n","import numpy as np\n","from sklearn.decomposition import PCA\n","import torch as t\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import gc\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1642892385575,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"},"user_tz":-540},"id":"aAr7GUJQsI4W"},"outputs":[],"source":["PATH=\"/content/drive/MyDrive/학교 프로젝트/2022 슈퍼챌린지 SW 해커톤\""]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7782,"status":"ok","timestamp":1642892393340,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"},"user_tz":-540},"id":"4NG4KFfaGrEO"},"outputs":[],"source":["def make_category(x):\n","  if int(x[1])<=3:\n","    return int(x[1])-1\n","  elif int(x[1])==4:\n","    return 1\n","  else:\n","    return int(x[1])-2\n","\n","with open(f\"{PATH}/data/train/감성대화말뭉치(최종데이터)_Training.json\",'r') as datafile:\n","    data=json.load(datafile)\n","train_data=pd.json_normalize(data)\n","train_data=pd.DataFrame(train_data)\n","data=\"\"\n","with open(f\"{PATH}/data/test/감성대화말뭉치(최종데이터)_Validation.json\",'r') as datafile:\n","    data=json.load(datafile)\n","test_data=pd.json_normalize(data)\n","test_data=pd.DataFrame(test_data)\n","\n","train_data[\"context\"]=train_data['talk.content.HS01']+\" \"+train_data['talk.content.HS02']+\" \"+train_data['talk.content.HS03']\n","test_data[\"context\"]=test_data['talk.content.HS01']+\" \"+test_data['talk.content.HS02']+\" \"+test_data['talk.content.HS03']\n","used_col=['profile.persona-id','profile.emotion.type', 'context']\n","for col in train_data.columns:\n","  if col not in used_col:\n","    train_data.drop(columns=col,inplace=True)\n","    test_data.drop(columns=col,inplace=True)\n","\n","train_data.columns=[\"id\",\"emotion\",\"context\"]\n","test_data.columns=[\"id\",\"emotion\",\"context\"]\n","\n","train_data[\"context\"]=train_data[\"context\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","train_data['context'].replace('', np.nan, inplace=True)\n","test_data[\"context\"]=test_data[\"context\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","test_data['context'].replace('', np.nan, inplace=True)\n","  \n","train_data[\"emotion\"]=train_data[\"emotion\"].apply(make_category)\n","test_data[\"emotion\"]=test_data[\"emotion\"].apply(make_category)"]},{"cell_type":"code","source":["test_data[\"emotion\"].unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ab9l6wxA9LB-","executionInfo":{"status":"ok","timestamp":1642892393343,"user_tz":-540,"elapsed":26,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"}},"outputId":"057cb115-d91a-4199-a673-95cda1a2b24c"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3, 4])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1642892393344,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"},"user_tz":-540},"id":"GE8Sj0nWKhov"},"outputs":[],"source":["train=train_data.values.tolist()\n","test=test_data.values.tolist()"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4725,"status":"ok","timestamp":1642892398047,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"},"user_tz":-540},"id":"rc9DGWJyG9I1","outputId":"fd6d3da5-cbe3-4930-cb10-d93e29b11850"},"outputs":[{"output_type":"stream","name":"stdout","text":["using cached model. /content/.cache/kobert_v1.zip\n","using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n","using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"]}],"source":["device = torch.device(\"cuda:0\")\n","\n","bertmodel, vocab = get_pytorch_kobert_model()\n","# 기본 Bert tokenizer 사용\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"]},{"cell_type":"code","source":["len(test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1EHXtqv9mdi3","executionInfo":{"status":"ok","timestamp":1642892398048,"user_tz":-540,"elapsed":22,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"}},"outputId":"d6bc3d7e-db9f-4cd9-d574-c71671d7421c"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5122"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1642892398049,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"},"user_tz":-540},"id":"tqSuvi2NHHnP","outputId":"85d17724-9518-479e-a02c-4c3751cedb60"},"outputs":[{"output_type":"stream","name":"stdout","text":["using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"]}],"source":["# 기본 Bert tokenizer 사용\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","# Setting parameters\n","max_len = 200 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n","batch_size = 12\n","warmup_ratio = 0.1\n","num_epochs = 100\n","max_grad_norm = 1\n","log_interval = 2000\n","learning_rate = 5e-5\n","threshold=3"]},{"cell_type":"markdown","source":["[model 참고 URL](https://zzaebok.github.io/deep_learning/nlp/Bert-for-classification/)"],"metadata":{"id":"kp8-wtd1N3P4"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"TdUY_IfYJaWq","executionInfo":{"status":"ok","timestamp":1642892407257,"user_tz":-540,"elapsed":9225,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"}}},"outputs":[],"source":["class emotionDataset(Dataset):\n","    ''' context&emotion dataset '''\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair) \n","        \n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","emotion_train_dataset = emotionDataset(train,2, 1, tok, max_len, True, False)\n","emotion_valid_dataset = emotionDataset(test,2, 1, tok, max_len, True, False)\n","\n","train_dataloader = DataLoader(emotion_train_dataset, batch_size=batch_size, num_workers=2)\n","valid_dataloader = DataLoader(emotion_valid_dataset, batch_size=batch_size, num_workers=2)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1642892407260,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"},"user_tz":-540},"id":"27VGThDyae9r","outputId":"23d9a2b8-80d1-41b5-e257-c0720fc3ab08"},"outputs":[{"output_type":"stream","name":"stdout","text":["40827 3403\n"]}],"source":["print(len(emotion_train_dataset),len(train_dataloader))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"w9mS9LD9yq2Q","executionInfo":{"status":"ok","timestamp":1642892410813,"user_tz":-540,"elapsed":3570,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"}}},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes = 5, # softmax 사용 <- binary일 경우는 2\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","      \n","model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"KpDHzHvyKJ8O","executionInfo":{"status":"ok","timestamp":1642892410819,"user_tz":-540,"elapsed":50,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"}}},"outputs":[],"source":["# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","# 옵티마이저 선언\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss() # softmax용 Loss Function 정하기 <- binary classification도 해당 loss function 사용 가능\n","\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"eu_spIhGKN7-","executionInfo":{"status":"ok","timestamp":1642892410820,"user_tz":-540,"elapsed":49,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"}}},"outputs":[],"source":["# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"]},{"cell_type":"code","source":["class EarlyStopping(object):\n","    def __init__(self, mode='min', min_delta=0, patience=3, percentage=False):\n","        self.mode = mode\n","        self.min_delta = min_delta\n","        self.patience = patience\n","        self.best = None\n","        self.num_bad_epochs = 0\n","        self.is_better = None\n","        self._init_is_better(mode, min_delta, percentage)\n","\n","        if patience == 0:\n","            self.is_better = lambda a, b: True\n","            self.step = lambda a: False\n","\n","    def step(self, metrics):\n","        if self.best is None:\n","            self.best = metrics\n","            return False\n","\n","        if np.isnan(metrics):\n","            return True\n","\n","        if self.is_better(metrics, self.best):\n","            self.num_bad_epochs = 0\n","            self.best = metrics\n","        else:\n","            self.num_bad_epochs += 1\n","\n","        if self.num_bad_epochs >= self.patience:\n","            print('terminating because of early stopping!')\n","            return True\n","\n","        return False\n","\n","    def _init_is_better(self, mode, min_delta, percentage):\n","        if mode not in {'min', 'max'}:\n","            raise ValueError('mode ' + mode + ' is unknown!')\n","        if not percentage:\n","            if mode == 'min':\n","                self.is_better = lambda a, best: a < best - min_delta\n","            if mode == 'max':\n","                self.is_better = lambda a, best: a > best + min_delta\n","        else:\n","            if mode == 'min':\n","                self.is_better = lambda a, best: a < best - (\n","                            best * min_delta / 100)\n","            if mode == 'max':\n","                self.is_better = lambda a, best: a > best + (\n","                            best * min_delta / 100)"],"metadata":{"id":"hbhhYq5_gy2g","executionInfo":{"status":"ok","timestamp":1642892410821,"user_tz":-540,"elapsed":48,"user":{"displayName":"choi yerim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08643437384388876422"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["3a6c1f942de1446eba72ec73184f9eb7","d29eed4258524fa4b7f215e6ab14f3b4","f00fc7c049ac4e26a26e2deac8dbc88f","d67bb54ac2f444c39342a373ca159834","cf2c1344d4714a6cb446e67b1733eb32","7725aec37df241d594ca8ad1936b339f","5c70c9f00e494460a04337d28dc80df1","20353b6d784e467ebb0d3e7c3b08d764","ff7c562ddb60480486f3defdaeafa9ff","f344f821b31a4e8f857a62d5c06d7c74","a7ce2f4ee706460f88f58ab98ee2e43b"]},"id":"Tdbci2cSKO7r","outputId":"5bec7fe0-fbbe-4dd3-dc05-55a9637c0be3"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myerim\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"output_type":"display_data","data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/yerim/2022SCHackathon/runs/3k8qiouz\" target=\"_blank\">dutiful-dawn-24</a></strong> to <a href=\"https://wandb.ai/yerim/2022SCHackathon\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  del sys.path[0]\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a6c1f942de1446eba72ec73184f9eb7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/3403 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 1 loss 1.7488702535629272 train acc 0.08333333333333333\n","epoch 1 batch id 2001 loss 1.3221477270126343 train acc 0.33608195902048976\n"]}],"source":["# 모델 학습 시작\n","previous_loss=-1\n","previous_acc=0\n","accm_down=0\n","wandb.init(project=\"2022SCHackathon\", entity=\"yerim\")\n","wandb.watch(model)\n","early_stopping = EarlyStopping(patience=3)\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    \n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    train_acc=train_acc / (batch_id+1)\n","    train_loss=loss.data.cpu().numpy()\n","    with torch.no_grad():\n","      model.eval() # 평가 모드로 변경\n","      for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(valid_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        test_acc += calc_accuracy(out, label)\n","      print(\"epoch {} testloss {} test acc {}\".format(e+1,loss.data.cpu().numpy(), test_acc / (batch_id+1)))\n","    wandb.log({\"epoch\":e,\"accuracy\": train_acc,\"val_accuracy\":test_acc/(batch_id+1),\"val_loss\":loss.data.cpu().numpy(),\"loss\":train_loss})\n","    if early_stopping.step(loss.data.cpu().numpy()):\n","        break  # early stop criterion is met, we can stop now\n"," #     torch.save(model,f'{PATH}/models/best_model_{e+1}epoch_maxlen_200.pt')"]},{"cell_type":"code","source":[""],"metadata":{"id":"odqku2ZOg7aI"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"koBERT_baseline.ipynb","provenance":[],"mount_file_id":"1q3E9d_nGr5QWx4hx7zKVOLeu5AsvKr6m","authorship_tag":"ABX9TyO0wPjPSwELMTTSiHSPR3bK"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3a6c1f942de1446eba72ec73184f9eb7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d29eed4258524fa4b7f215e6ab14f3b4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f00fc7c049ac4e26a26e2deac8dbc88f","IPY_MODEL_d67bb54ac2f444c39342a373ca159834","IPY_MODEL_cf2c1344d4714a6cb446e67b1733eb32"]}},"d29eed4258524fa4b7f215e6ab14f3b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f00fc7c049ac4e26a26e2deac8dbc88f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7725aec37df241d594ca8ad1936b339f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 62%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5c70c9f00e494460a04337d28dc80df1"}},"d67bb54ac2f444c39342a373ca159834":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_20353b6d784e467ebb0d3e7c3b08d764","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":3403,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2100,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ff7c562ddb60480486f3defdaeafa9ff"}},"cf2c1344d4714a6cb446e67b1733eb32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f344f821b31a4e8f857a62d5c06d7c74","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2100/3403 [10:05&lt;06:14,  3.48it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a7ce2f4ee706460f88f58ab98ee2e43b"}},"7725aec37df241d594ca8ad1936b339f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5c70c9f00e494460a04337d28dc80df1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"20353b6d784e467ebb0d3e7c3b08d764":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ff7c562ddb60480486f3defdaeafa9ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f344f821b31a4e8f857a62d5c06d7c74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a7ce2f4ee706460f88f58ab98ee2e43b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}